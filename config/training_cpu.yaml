# config/training_cpu.yaml

# CPU-only training configuration.
# Inherits from base.yaml. For debugging, testing, and no-GPU environments.

_base_: base.yaml

# All settings are under the 'training' key to correctly override the base config.
training:
  batch_size: 4
  accumulation_steps: 1 # Accumulation is less relevant on CPU.
  effective_batch_size: 4
  lr: 1e-4 # Low learning rate for stable CPU training.
  gradient_checkpointing: false # Not effective on CPU.
  cpu_offload: false            # Irrelevant.
  activation_offload: false     # Irrelevant.
  compile_model: false          # torch.compile is primarily for GPUs.
  dtype: "float32"              # CPU training must use float32.
  flash_attention: false        # GPU-only feature.

# Distributed training on CPU uses the 'gloo' backend.
distributed:
  backend: "gloo" # This must be changed from 'nccl' for CPU distributed training.
  gradient_as_bucket_view: false