# config/training_colab_free.yaml

# Google Colab (Free Tier) specific configuration (e.g., K80/T4 with ~12-15GB VRAM)
# Inherits from base.yaml. This is the most memory-conservative setup.

_base_: base.yaml

# Absolute minimum batch size.
training:
  batch_size: 4
  gradient_accumulation: 32 # Maximum accumulation.
  effective_batch_size: 128

# Very low learning rate for training stability on older hardware.
optimization:
  learning_rate: 1e-4

# All possible memory saving features are turned on.
memory:
  gradient_checkpointing: true
  cpu_offload: true
  activation_offload: true
  compile_model: false          # Compiling can be slow or fail on older GPUs.
  dtype: float16                # float16 is the only safe option (K80 doesn't support bfloat16).
  use_flash_attention: false    # K80 does not support Flash Attention.
  max_split_size: 64            # Tiny memory splits.
  empty_cache_freq: 25          # Very frequent memory cleanup.

distributed:
  # Distributed training is not applicable in a standard Colab notebook.
  gradient_as_bucket_view: false