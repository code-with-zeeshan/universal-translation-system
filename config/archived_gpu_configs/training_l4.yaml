# config/training_l4.yaml

# L4-specific configuration (24GB memory, Ada Lovelace Architecture)
# Inherits from base.yaml. Optimized for efficiency, not raw training speed.

_base_: base.yaml

# All settings are under the 'training' key to correctly override the base config.
training:
  batch_size: 16
  accumulation_steps: 8
  effective_batch_size: 128
  lr: 2e-4 # Use a conservative learning rate for stability.
  gradient_checkpointing: true
  cpu_offload: false
  activation_offload: false
  compile_mode: "default" # A safe compilation mode for this efficiency-focused GPU.
  dtype: "bfloat16"         # Ada architecture supports bfloat16.
  max_split_size: 256

distributed:
  gradient_as_bucket_view: false