# config/base.yaml
# Base configuration - shared across all GPU types
# This file contains common settings to reduce duplication

config_version: "1.0"

# Data configuration
data:
  processed_dir: data/processed
  checkpoint_dir: "checkpoints/default_run" # Added for better organization
  model_dir: "models/production"
  languages:
    - en
    - es
    - fr
    - de
    - zh
    - ja
    - ko
    - ar
    - hi
    - ru
    - pt
    - it
    - tr
    - th
    - vi
    - pl
    - uk
    - nl
    - id
    - sv
  
  training_distribution:
    en-es: 2000000
    en-fr: 2000000
    en-de: 2000000
    en-zh: 1500000
    en-ru: 1500000
    en-ja: 1000000
    en-ar: 1000000
    en-pt: 1000000
    en-it: 1000000
    en-hi: 500000
    en-ko: 500000
    en-tr: 500000
    en-th: 300000
    en-vi: 300000
    en-pl: 300000
    en-uk: 300000
    en-nl: 300000
    en-id: 300000
    en-sv: 300000
    es-pt: 200000
    zh-ja: 200000
    fr-es: 200000
    de-fr: 200000
    ru-uk: 200000
  
  vocabulary_strategy:
    approach: production
    groups:
      latin: [en, es, fr, de, it, pt, nl, sv, pl, id, vi, tr]
      cjk: [zh, ja, ko]
      arabic: [ar]
      devanagari: [hi]
      cyrillic: [ru, uk]
      thai: [th]

# Model architecture
model:
  hidden_dim: 1024
  num_layers: 6
  num_heads: 16
  decoder_dim: 512
  decoder_layers: 6
  decoder_heads: 8
  dropout: 0.1
  max_vocab_size: 50000

# Common training settings
training:
  # High-level strategy
  use_fsdp: true
  num_epochs: 20
  batch_size: 32
  accumulation_steps: 4
  effective_batch_size: 128

  # Hyperparameters
  lr: 5e-4
  weight_decay: 0.01
  warmup_steps: 4000
  max_grad_norm: 1.0
  
  # Memory & Performance Optimizations
  mixed_precision: true
  dtype: "bfloat16"
  gradient_checkpointing: true
  cpu_offload: false
  activation_offload: false
  compile_model: true
  compile_mode: "reduce-overhead"
  flash_attention: true
  use_fp8: false
  max_split_size: 512
  empty_cache_freq: 100

  # Distributed Training Settings
  sharding_strategy: "FULL_SHARD"
  backward_prefetch: "BACKWARD_PRE"
  
  # Logging and Saving
  save_every: 2
  validate_every: 1
  log_every: 50
  profile_training: false # Added missing parameter
  max_sentence_length: 50
  dynamic_vocabulary: true
  vocab_switch_penalty: 0.001
  # The following keys were present but are better handled by the trainer logic
  # dynamic_batch_size: true
  # adam_beta1: 0.9
  # adam_beta2: 0.98
  # adam_epsilon: 1e-8
  # pin_memory: true
  # persistent_workers: true
  # prefetch_factor: 4
  # use_channels_last: true
  # use_fused_optimizer: true
  # enable_nested_tensor: true
  # use_inductor: true
  # profile_memory: false
  # use_safetensors: true

# Distributed training defaults
distributed:
  backend: nccl
  find_unused_parameters: false
  broadcast_buffers: false
  bucket_cap_mb: 25
  gradient_as_bucket_view: true
  static_graph: true

# Monitoring and logging
monitoring:
  use_wandb: true
  use_tensorboard: false
  log_gradients: false
  log_weights: false
  log_learning_rate: true    