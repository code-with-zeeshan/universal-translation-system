# config/base.yaml
# Base configuration - shared across all GPU types
# This file contains common settings to reduce duplication

config_version: "1.0.0"
data_version: "1.0.0"
pipeline_version: "1.0.0"

# Data configuration
data:
  max_sentence_length: 50
  output_dir: "data/processed"
  total_size_gb: 8
  processed_dir: data/processed
  checkpoint_dir: "checkpoints/default_run" # Added for better organization
  model_dir: "models/production"
  languages:
    - en
    - es
    - fr
    - de
    - zh
    - ja
    - ko
    - ar
    - hi
    - ru
    - pt
    - it
    - tr
    - th
    - vi
    - pl
    - uk
    - nl
    - id
    - sv
  
  training_distribution:
    en-es: 2000000
    en-fr: 2000000
    en-de: 2000000
    en-zh: 1500000
    en-ru: 1500000
    en-ja: 1000000
    en-ar: 1000000
    en-pt: 1000000
    en-it: 1000000
    en-hi: 500000
    en-ko: 500000
    en-tr: 500000
    en-th: 300000
    en-vi: 300000
    en-pl: 300000
    en-uk: 300000
    en-nl: 300000
    en-id: 300000
    en-sv: 300000
    es-pt: 200000
    zh-ja: 200000
    fr-es: 200000
    de-fr: 200000
    ru-uk: 200000
  
  vocabulary_strategy:
    approach: production # or "research"
    groups:
      latin: [en, es, fr, de, it, pt, nl, sv, pl, id, vi, tr]
      cjk: [zh, ja, ko]
      arabic: [ar]
      devanagari: [hi]
      cyrillic: [ru, uk]
      thai: [th]

data_strategy:
  priority_rules:
    high: ['en-es', 'en-fr', 'en-de', 'en-zh', 'en-ru']
    medium: ['en-ja', 'en-ar', 'en-pt', 'en-it', 'en-hi', 'en-ko',
             'es-pt', 'es-fr', 'de-fr', 'zh-ja', 'ar-fr', 'ru-uk']
    low: ['en-tr', 'en-th', 'en-vi', 'en-pl', 'en-uk', 'en-nl', 'en-id', 'en-sv']
  
  source_preferences:
    # Recommended sources by pair type
    en_centric: ['opus-100', 'nllb-seed', 'ccmatrix', 'wmt']
    european: ['opus-100', 'wmt', 'opus_books']
    asian: ['opus-100', 'nllb-seed', 'ted_talks']
    default: ['opus-100', 'nllb-seed', 'tatoeba']


# Model architecture
model:
  hidden_dim: 1024
  num_layers: 6
  num_heads: 16
  decoder_dim: 512
  decoder_layers: 6
  decoder_heads: 8
  dropout: 0.1
  max_vocab_size: 50000

# Common training settings
training:
  # High-level strategy
  use_fsdp: true
  num_epochs: 20
  batch_size: 32
  accumulation_steps: 4
  effective_batch_size: 128

  # Hyperparameters
  lr: 5e-4
  weight_decay: 0.01
  warmup_steps: 4000
  max_grad_norm: 1.0
  
  # Memory & Performance Optimizations
  mixed_precision: true
  dtype: "bfloat16"
  gradient_checkpointing: true
  cpu_offload: false
  activation_offload: false
  compile_model: true
  compile_mode: "reduce-overhead"
  flash_attention: true
  use_fp8: false
  max_split_size: 512
  empty_cache_freq: 100

  # Distributed Training Settings
  sharding_strategy: "FULL_SHARD"
  backward_prefetch: "BACKWARD_PRE"
  
  # Logging and Saving
  save_every: 2
  validate_every: 1
  log_every: 50
  profile_training: false # Added missing parameter
  max_sentence_length: 50
  dynamic_vocabulary: true
  vocab_switch_penalty: 0.001
  # The following keys were present but are better handled by the trainer logic
  # dynamic_batch_size: true
  # adam_beta1: 0.9
  # adam_beta2: 0.98
  # adam_epsilon: 1e-8
  # pin_memory: true
  # persistent_workers: true
  # prefetch_factor: 4
  # use_channels_last: true
  # use_fused_optimizer: true
  # enable_nested_tensor: true
  # use_inductor: true
  # profile_memory: false
  # use_safetensors: true
  language_to_pack_mapping:
    en: latin
    es: latin
    fr: latin
    de: latin
    it: latin
    pt: latin
    nl: latin
    sv: latin
    pl: latin
    id: latin
    vi: latin
    tr: latin
    zh: cjk
    ja: cjk
    ko: cjk
    ar: arabic
    hi: devanagari
    ru: cyrillic
    uk: cyrillic
    th: thai


  dynamic_vocabulary: true
  vocab_switch_penalty: 0.001  # Small penalty for switching  

# Distributed training defaults
distributed:
  backend: nccl
  find_unused_parameters: false
  broadcast_buffers: false
  bucket_cap_mb: 25
  gradient_as_bucket_view: true
  static_graph: true

# Monitoring and logging
monitoring:
  use_wandb: true
  use_tensorboard: false
  log_gradients: false
  log_weights: false
  log_learning_rate: true   

security:
  trusted_model_sources:
    - facebook/
    - microsoft/
    - google/
    - Helsinki-NLP/
  enable_path_validation: true
  max_file_size_gb: 10   