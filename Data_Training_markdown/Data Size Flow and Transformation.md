## Data Size Flow and Transformation

The pipeline transforms and filters data at each stage. Here's how the data flows and transforms:

### ğŸ“Š Data Size Transformation Table

| Stage | Size | Status | What Happens | Kept/Discarded |
|-------|------|--------|--------------|----------------|
| **1. Essential Data** | ~100MB | âœ… **Kept Separate** | Evaluation sets | **Kept for testing** |
| **2. Raw Training Data** | ~50GB | âŒ **Temporary** | Downloaded raw data | **Discarded after sampling** |
| **3. Sampled Data** | ~5GB | âœ… **Kept** | High-quality filtered | **Part of final 8GB** |
| **4. Augmented Data** | ~3GB | âœ… **Kept** | Synthetic additions | **Part of final 8GB** |
| **5. Final Training Data** | **~8GB** | âœ… **Final Output** | Sampled + Augmented | **This is what you train on** |

### ğŸ”„ Data Pipeline Transformation Flow

```
Step 1: Download Raw Data (~50GB)
         â†“ [Quality Filtering & Sampling]
Step 2: High-Quality Samples (~5GB) â† Discard ~45GB of low quality
         â†“ [Keep]
Step 3: Generate Synthetic Data (~3GB)
         â†“ [Merge]
Step 4: Final Training Corpus (~8GB)

Evaluation Data (~100MB) â†’ Kept separately for testing
```

### ğŸ“ Final Directory Structure and Sizes

| Directory | Contents | Size | Fate |
|-----------|----------|------|------|
| `data/essential/` | FLORES-200, Tatoeba samples | ~100MB | âœ… **Kept** (for evaluation) |
| `data/raw/` | Original downloads | ~50GB | âŒ **Can be deleted** |
| `data/sampled/` | Quality-filtered pairs | ~5GB | âš ï¸ **Intermediate** (can delete after final) |
| `data/final/` | Augmented + pivot pairs | ~3GB | âš ï¸ **Intermediate** (can delete after final) |
| **`data/processed/`** | **Final training corpus** | **~8GB** | âœ… **KEEP - This is your dataset** |

### ğŸ’¾ Storage Requirements During Processing

| Phase | Storage Needed | Explanation |
|-------|----------------|-------------|
| During download | ~60GB | Raw data (50GB) + workspace |
| During sampling | ~55GB | Raw (50GB) + sampled (5GB) |
| During augmentation | ~13GB | Sampled (5GB) + augmented (3GB) + workspace |
| **Final (minimum)** | **~8.1GB** | Training (8GB) + evaluation (0.1GB) |
| **Final (recommended)** | **~13GB** | Keep sampled/augmented for debugging |

### ğŸ¯ What You Actually Get

```yaml
Final Dataset (8GB):
â”œâ”€â”€ High-quality sampled pairs: ~5GB
â”‚   â”œâ”€â”€ en-es: 2M sentences (filtered from ~10M)
â”‚   â”œâ”€â”€ en-fr: 2M sentences (filtered from ~10M)
â”‚   â””â”€â”€ ... other pairs
â”‚
â””â”€â”€ Synthetic augmented pairs: ~3GB
    â”œâ”€â”€ Backtranslated pairs
    â””â”€â”€ Pivot-generated pairs

Evaluation Set (100MB) - Separate:
â”œâ”€â”€ FLORES-200: High-quality test/dev sets
â””â”€â”€ Tatoeba: Additional evaluation pairs
```

### ğŸ—‘ï¸ Space Optimization Timeline

```bash
# After downloads complete
rm -rf data/raw/  # Free ~50GB

# After final corpus generation
rm -rf data/sampled/  # Free ~5GB (optional)
rm -rf data/final/    # Free ~3GB (optional)

# Minimum space after cleanup: ~8.1GB
```

**Summary**: The 50GB raw data is filtered down to 5GB of high-quality pairs, then augmented with 3GB of synthetic data, resulting in a final 8GB training corpus. The large raw downloads can be deleted after processing.