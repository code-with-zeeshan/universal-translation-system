# üôè Acknowledgments

The Universal Translation System would not be possible without the incredible work of the open-source community and the researchers who have paved the way for modern machine translation. We are deeply grateful for their contributions.

## Models & Research

### NLLB-200 by Meta AI
- **Repository**: [github.com/facebookresearch/fairseq/tree/nllb](https://github.com/facebookresearch/fairseq/tree/nllb)
- **Contribution**: Inspiration and pretrained models for multilingual translation
- **Paper**: [No Language Left Behind: Scaling Human-Centered Machine Translation](https://arxiv.org/abs/2207.04672)

### XLM-RoBERTa
- **Model**: [huggingface.co/xlm-roberta-base](https://huggingface.co/xlm-roberta-base)
- **Contribution**: Encoder initialization and cross-lingual representations
- **Paper**: [Unsupervised Cross-lingual Representation Learning at Scale](https://arxiv.org/abs/1911.02116)

### mBART
- **Repository**: [github.com/pytorch/fairseq/tree/master/examples/mbart](https://github.com/pytorch/fairseq/tree/master/examples/mbart)
- **Contribution**: Decoder architecture insights and multilingual denoising pretraining
- **Paper**: [Multilingual Denoising Pre-training for Neural Machine Translation](https://arxiv.org/abs/2001.08210)

## Libraries & Tools

### Core Frameworks

#### PyTorch
- **Website**: [pytorch.org](https://pytorch.org/)
- **Contribution**: Deep learning framework powering our models
- **License**: BSD-3-Clause

#### Transformers by Hugging Face
- **Repository**: [github.com/huggingface/transformers](https://github.com/huggingface/transformers)
- **Contribution**: Model implementations and training utilities
- **License**: Apache 2.0

### Inference & Optimization

#### ONNX Runtime
- **Website**: [onnxruntime.ai](https://onnxruntime.ai/)
- **Contribution**: Efficient mobile inference
- **License**: MIT

#### Litserve
- **Repository**: [github.com/Lightning-AI/litserve](https://github.com/Lightning-AI/litserve)
- **Contribution**: High-performance AI model serving (2x faster than FastAPI)
- **License**: Apache 2.0

### Tokenization & Text Processing

#### SentencePiece
- **Repository**: [github.com/google/sentencepiece](https://github.com/google/sentencepiece)
- **Contribution**: Language-agnostic tokenization
- **License**: Apache 2.0

### Monitoring & Infrastructure

#### Prometheus
- **Website**: [prometheus.io](https://prometheus.io/)
- **Contribution**: Metrics collection and monitoring
- **License**: Apache 2.0

#### Grafana
- **Website**: [grafana.com](https://grafana.com/)
- **Contribution**: Metrics visualization and dashboards
- **License**: AGPL-3.0

## Data Sources

### OPUS - Open Parallel Corpus
- **Website**: [opus.nlpl.eu](https://opus.nlpl.eu/)
- **Contribution**: Massive parallel corpora for training
- **Maintainer**: NLPL (Nordic Language Processing Laboratory)

### Tatoeba
- **Website**: [tatoeba.org](https://tatoeba.org/)
- **Contribution**: Community-driven translations and sentence pairs
- **License**: CC BY 2.0 FR

### FLORES-200
- **Repository**: [github.com/facebookresearch/flores](https://github.com/facebookresearch/flores)
- **Contribution**: High-quality evaluation dataset for 200 languages
- **Paper**: [The FLORES-200 Evaluation Benchmark](https://arxiv.org/abs/2207.04672)

## Community Contributors

### Special Thanks To:

- The **PyTorch** team for creating an amazing deep learning framework
- The **Hugging Face** team for democratizing NLP and making models accessible
- **Meta AI Research** for open-sourcing NLLB-200 and advancing multilingual NLP
- The **OPUS** project maintainers for curating parallel corpora
- The **Tatoeba** community for their collaborative translation efforts
- All **open-source contributors** who have submitted issues, PRs, and feedback

### Individual Contributors

*This section will be updated as the project grows. If you've contributed and would like to be listed here, please submit a PR!*

## Research Papers

We also acknowledge the following seminal papers that have influenced our approach:

1. **Attention Is All You Need** (Vaswani et al., 2017) - Transformer architecture
2. **BERT: Pre-training of Deep Bidirectional Transformers** (Devlin et al., 2018) - Bidirectional encoding
3. **Scaling Laws for Neural Language Models** (Kaplan et al., 2020) - Model scaling insights
4. **Language Models are Few-Shot Learners** (Brown et al., 2020) - Large language model capabilities

## Supporting Organizations

- **GitHub** for hosting our code and enabling collaboration
- **PyPI** for Python package distribution
- **Docker Hub** for container hosting
- All **cloud providers** offering free tiers that help developers test and deploy

---

## How to Add Your Acknowledgment

If your work has been used in this project and you'd like to be acknowledged, or if you've contributed significantly, please:

1. Submit a PR adding your information to this file
2. Include:
   - Project/Paper name
   - Link to repository/website
   - Brief description of contribution
   - License information (if applicable)

We strive to give credit where credit is due. If we've missed anyone, please let us know!

---

*Last updated: [July 25 2025]*

**Thank you to everyone who has made this project possible! üôè**